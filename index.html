<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
</head>
<body>
    <h1>Welcome to Your Shifting Session</h1>
    <p>Make sure you're in a quiet, comfortable space.</p>
    <button id="startButton">Start Session</button>

    <script>
        const audio = new Audio();

        document.getElementById('startButton').addEventListener('click', async () => {
            console.log("Session starting...");

            // First instruction: Take a deep breath in and out
            await playText("Take a deep breath in and out.");
            console.log("First message played");

            // Short pause (5 seconds)
            await new Promise(resolve => setTimeout(resolve, 5000));
            console.log("Pause complete");

            // Second instruction and prompt for user input
            await playText("What are you noticing? Is the mind quieting down or are other things happening?");
            console.log("Second message played");

            // Capture user input with real audio recording and transcription
            const userResponse = await captureUserInput();
            console.log(`User response: ${userResponse}`);

            // Get GPT's response and play it
            const gptResponse = await getGptResponse(userResponse);
            await playText(gptResponse);
            console.log("GPT response played");
        });

        async function playText(text) {
            try {
                const url = `https://texttospeech.googleapis.com/v1/text:synthesize?key=YOUR_GOOGLE_API_KEY`;

                const response = await fetch(url, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        input: { text: text },
                        voice: { languageCode: "en-US", name: "en-US-Wavenet-D" },
                        audioConfig: { audioEncoding: "MP3" }
                    })
                });

                const data = await response.json();
                const audioContent = data.audioContent;

                audio.src = `data:audio/mp3;base64,${audioContent}`;
                audio.load();
                audio.play();

                return new Promise(resolve => {
                    audio.onended = resolve;
                });
            } catch (error) {
                console.error("Error in playText function:", error);
            }
        }

        async function captureUserInput() {
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                const recorder = new MediaRecorder(stream);
                const chunks = [];

                // Start recording
                recorder.ondataavailable = event => chunks.push(event.data);
                recorder.start();
                console.log("Recording audio...");

                // Stop recording after 5 seconds (or adjust as needed)
                await new Promise(resolve => setTimeout(resolve, 5000));
                recorder.stop();

                // Wait for recording to complete
                const audioBlob = await new Promise(resolve => {
                    recorder.onstop = () => {
                        const blob = new Blob(chunks, { type: 'audio/webm' });
                        resolve(blob);
                    };
                });

                // Convert the audio to base64 for Google Speech-to-Text API
                const audioBase64 = await blobToBase64(audioBlob);

                // Call Google Speech-to-Text API
                const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio: { content: audioBase64 },
                        config: { encoding: "WEBM_OPUS", sampleRateHertz: 48000, languageCode: "en-US" }
                    })
                });

                const data = await response.json();
                const transcription = data.results?.[0]?.alternatives?.[0]?.transcript || "No response detected.";
                return transcription;

            } catch (error) {
                console.error("Error capturing or transcribing audio:", error);
                return "Error capturing audio.";
            }
        }

        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => resolve(reader.result.split(",")[1]);
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }

        async function getGptResponse(userText) {
            // Simulated GPT response (replace with actual GPT API call)
            return new Promise(resolve => {
                setTimeout(() => resolve("Great, we are complete"), 2000);
            });
        }
    </script>
</body>
</html>
