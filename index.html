<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #F5F5DC; /* Soft beige background for calmness */
            color: #333;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            margin: 0;
            padding: 20px;
        }

        h1, h2 {
            font-family: 'Georgia', serif;
            color: #4A8A7F; /* Soothing green */
        }

        p {
            font-size: 1em;
            color: #555;
            max-width: 800px;
            text-align: center;
            line-height: 1.6;
        }

        #overview {
            background-color: #e0e7da; /* Slightly darker background for contrast */
            border-radius: 8px;
            padding: 15px 20px;
            margin-bottom: 20px;
            box-shadow: 0px 2px 5px rgba(0, 0, 0, 0.1);
            text-align: left;
            max-width: 800px;
        }

        #startButton {
            font-size: 1.1em;
            color: #fff;
            background-color: #4A8A7F; /* Matching button color */
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
            margin-top: 20px;
        }

        #startButton.active {
            background-color: #FF6347; /* Red when active */
        }

        #log {
            white-space: pre-wrap;
            background-color: #e8e8e8;
            padding: 15px;
            border-radius: 5px;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 20px;
            width: 80%;
            font-family: monospace;
            font-size: 0.9em;
            color: #333;
        }

        .background-image {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: url('https://www.example.com/your-calm-background.jpg'); /* Replace with a suitable image URL */
            background-size: cover;
            background-position: center;
            opacity: 0.2;
            z-index: -1;
        }
    </style>
</head>
<body>
    <div class="background-image"></div>

    <div id="overview">
        <h2>Product Overview</h2>
        <p><strong>Last Update:</strong> October 25, 2024</p>
        <p><strong>Current Features:</strong>
            <ul>
                <li>Automated audio-guided meditation with dynamically timed user interactions.</li>
                <li>Real-time speech detection and transcription using Google Speech-to-Text API.</li>
                <li>Analysis of user responses using GPT to interpret if the user feels calm or not.</li>
                <li>Microphone access for capturing user feedback, with dynamic silence detection for optimal user experience.</li>
                <li>Smooth integration of Google Cloud and OpenAI APIs for processing user data.</li>
            </ul>
        </p>
        <p><strong>Product Architecture:</strong> The front end is a simple HTML, CSS, and JavaScript page deployed using Vercel, which also hosts serverless functions for handling API requests. Key serverless functions include:
            <ul>
                <li><strong>googleApiKey.js:</strong> Manages retrieval of the Google Speech-to-Text API key.</li>
                <li><strong>OpenaiRequest.js:</strong> Communicates with the OpenAI GPT API to analyze user transcriptions.</li>
            </ul>
        </p>
        <p>The productâ€™s workflow involves guiding users through a relaxation process, capturing their responses, and adjusting the session based on user feedback, all while ensuring that users can keep their eyes closed throughout the experience.</p>
        <p>This version includes improved error handling, detailed logging, and CORS configurations for seamless API communication.</p>
    </div>

    <div class="container">
        <h1>Welcome to Your Shifting Session</h1>
        <p>Please ensure you're in a quiet, comfortable space to begin your journey.</p>
        <button id="startButton" onclick="toggleSession()">Start Session</button>
        <div id="log"></div>
    </div>

    <script>
        const logElement = document.getElementById("log");
        const startButton = document.getElementById("startButton");
        let audioContext, audioBuffers = {}, mediaStream, googleApiKey, silenceDetectionActive = true;

        function logMessage(message) {
            logElement.textContent += message + "\n";
            logElement.scrollTop = logElement.scrollHeight;
        }

        async function initAudio() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const audioFiles = ['Intro', 'Prompt1', 'Calm_response', 'Supportive_response'];
                for (const file of audioFiles) {
                    audioBuffers[file] = await loadAudioBuffer(file);
                }
                logMessage("Audio buffers loaded.");
            } catch (error) {
                logMessage("Error initializing audio context or loading buffers: " + error);
            }
        }

        async function loadAudioBuffer(fileName) {
            try {
                const response = await fetch(`https://raw.githubusercontent.com/Jsskieracer/Shifting-mvp/main/audio/${fileName}.mp3`);
                const arrayBuffer = await response.arrayBuffer();
                return await audioContext.decodeAudioData(arrayBuffer);
            } catch (error) {
                logMessage(`Error loading audio buffer for ${fileName}: ${error}`);
            }
        }

        async function playAudioBuffer(bufferName, delayAfter = 0) {
            try {
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffers[bufferName];
                source.connect(audioContext.destination);
                source.start();

                logMessage(`${bufferName} started playing.`);

                const bufferDuration = source.buffer.duration;
                await new Promise(resolve => setTimeout(resolve, (bufferDuration + delayAfter) * 1000));
                logMessage(`${bufferName} finished playing. Waiting for ${delayAfter} seconds.`);
            } catch (error) {
                logMessage(`Error playing ${bufferName}: ${error}`);
            }
        }

        async function toggleSession() {
            if (startButton.classList.contains("active")) {
                endSession();
                return;
            }
            startButton.classList.add("active");
            startButton.textContent = "Stop Session";

            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                await audioContext.resume();
                logMessage("Audio context resumed.");
            }

            logMessage("Session starting...");
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                logMessage("Microphone access granted");
            } catch (error) {
                logMessage("Microphone access denied: " + error);
                return;
            }

            googleApiKey = await fetchGoogleApiKey();
            if (!googleApiKey) {
                logMessage("Google API key could not be retrieved. Ending session.");
                endSession();
                return;
            }

            await initAudio();
            await playAudioBuffer('Intro');
            await new Promise(resolve => setTimeout(resolve, 3000));
            logMessage("3-second delay completed after Intro.");
            await playAudioBuffer('Prompt1');

            logMessage("Prompt 1 segment finished; starting dynamic recording...");
            const userResponse = await captureDynamicInput();
            logMessage("User response captured: " + userResponse);

            const isCalm = await analyzeWithGPT(userResponse);
            if (isCalm) {
                logMessage("GPT analysis: User feels calm.");
                await playAudioBuffer('Calm_response');
            } else {
                logMessage("GPT analysis: User does not feel calm.");
                await playAudioBuffer('Supportive_response');
            }

            endSession();
        }

        function endSession() {
            silenceDetectionActive = false;
            startButton.classList.remove("active");
            startButton.textContent = "Start Session";
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                logMessage("Microphone turned off.");
            }
            logMessage("Session complete; stopping silence detection logging.");
        }

        async function fetchGoogleApiKey() {
            try {
                const response = await fetch("https://shifting-mvp.vercel.app/api/googleApiKey", {
                    method: "GET",
                    headers: {
                        "Content-Type": "application/json",
                        "Accept": "application/json"
                    },
                    mode: "cors"
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! Status: ${response.status}`);
                }

                const data = await response.json();
                return data.googleApiKey;
            } catch (error) {
                logMessage("Error fetching Google API key: " + error.message);
                return null;
            }
        }

        async function analyzeWithGPT(transcription) {
            try {
                logMessage("Starting GPT analysis for transcription: " + transcription);
                const response = await fetch("https://shifting-mvp.vercel.app/api/OpenaiRequest", {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify({ transcription }),
                });

                if (!response.ok) {
                    throw new Error(`Error analyzing response with GPT: HTTP ${response.status} ${response.statusText}`);
                }

                const data = await response.json();
                const gptText = data.text?.trim().toLowerCase();
                logMessage("GPT analysis response: " + gptText);
                return gptText === "calm";
            } catch (error) {
                logMessage("Error analyzing response with GPT: " + error.message);
                return false;
            }
        }
    </script>
</body>
</html>
