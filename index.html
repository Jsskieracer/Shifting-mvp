<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
    <style>
        #log { 
            white-space: pre-wrap; 
            background-color: #f0f0f0; 
            padding: 10px; 
            border-radius: 5px; 
            max-height: 400px; /* Increased height for easier copying */
            overflow-y: auto; 
            margin-top: 20px; 
        }
    </style>
</head>
<body>
    <h1>Welcome to Your Shifting Session</h1>
    <p>Make sure you're in a quiet, comfortable space and your AirPods are connected.</p>
    <button id="startButton">Start Session</button>
    <div id="log"></div>

    <script>
        const logElement = document.getElementById("log");

        function logMessage(message) {
            logElement.textContent += message + "\n";
            logElement.scrollTop = logElement.scrollHeight;
        }

        let audioContext, audioBuffers = {}, mediaStream;

        // Initialize Web Audio API and load audio buffers
        async function initAudio() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const audioFiles = ['Intro', 'Prompt1', 'Calm_response', 'Supportive_response'];
                for (const file of audioFiles) {
                    audioBuffers[file] = await loadAudioBuffer(file);
                }
                logMessage("Audio buffers loaded.");
            } catch (error) {
                logMessage("Error initializing audio context or loading buffers: " + error);
            }
        }

        // Load an audio file as a buffer
        async function loadAudioBuffer(fileName) {
            try {
                const response = await fetch(`https://raw.githubusercontent.com/Jsskieracer/Shifting-mvp/main/audio/${fileName}.mp3`);
                const arrayBuffer = await response.arrayBuffer();
                return await audioContext.decodeAudioData(arrayBuffer);
            } catch (error) {
                logMessage(`Error loading audio buffer for ${fileName}: ${error}`);
            }
        }

        // Play an audio buffer and add an explicit delay after it completes
        async function playAudioBuffer(bufferName, delayAfter = 0) {
            try {
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffers[bufferName];
                source.connect(audioContext.destination);
                source.start();

                logMessage(`${bufferName} started playing.`);

                // Wait for the buffer's duration + delayAfter before returning
                const bufferDuration = source.buffer.duration;
                await new Promise(resolve => setTimeout(resolve, (bufferDuration + delayAfter) * 1000));
                logMessage(`${bufferName} finished playing. Waiting for ${delayAfter} seconds.`);
            } catch (error) {
                logMessage(`Error playing ${bufferName}: ${error}`);
            }
        }

        document.getElementById('startButton').addEventListener('click', async () => {
            if (!audioContext) {
                // Ensure audio context is resumed on first interaction
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                await audioContext.resume();
                logMessage("Audio context resumed.");
            }

            logMessage("Session starting...");

            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                logMessage("Microphone access granted");
            } catch (error) {
                logMessage("Microphone access denied: " + error);
                return;
            }

            // Initialize audio buffers and start session flow
            await initAudio();

            // Play Intro, then wait 3 seconds before playing Prompt1
            await playAudioBuffer('Intro');
            await new Promise(resolve => setTimeout(resolve, 3000)); // Explicit 3-second delay
            logMessage("3-second delay completed after Intro.");
            await playAudioBuffer('Prompt1');

            // After Prompt1, simulate transcription handling
            logMessage("Simulating transcription processing...");
            setTimeout(async () => {
                logMessage("Simulated transcription result: 'calm detected'");
                await playAudioBuffer('Calm_response');
            }, 5000); // Simulate a delay before playing Calm_response
        });
    </script>
</body>
</html>
