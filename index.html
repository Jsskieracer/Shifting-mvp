<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #F5F5DC; /* Soft beige background for calmness */
            color: #333;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        h1 {
            font-family: 'Georgia', serif;
            color: #4A8A7F; /* Soothing green */
            font-size: 2.5em;
        }

        p {
            font-size: 1.2em;
            color: #555;
            max-width: 600px;
            text-align: center;
            line-height: 1.6;
        }

        button {
            font-size: 1.1em;
            color: #fff;
            background-color: #4A8A7F; /* Matching button color */
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
            margin-top: 20px;
        }

        button:hover {
            background-color: #3B6D63; /* Darker green on hover */
        }

        #log {
            white-space: pre-wrap;
            background-color: #e8e8e8;
            padding: 15px;
            border-radius: 5px;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 20px;
            width: 80%;
            font-family: monospace;
            font-size: 0.9em;
            color: #333;
        }

        .container {
            text-align: center;
            padding: 20px;
            max-width: 600px;
        }

        .background-image {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: url('https://unsplash.com/photos/txx9nH_8nd8'); /* Replace with selected image URL */
            background-size: cover;
            background-position: center;
            opacity: 0.2;
            z-index: -1;
        }
    </style>
</head>
<body>
    <div class="background-image"></div>
    <div class="container">
        <h1>Welcome to Your Shifting Session</h1>
        <p>Please ensure you're in a quiet, comfortable space to begin your journey.</p>
        <button id="startButton">Start Session</button>
        <div id="log"></div>
    </div>

    <script>
        const logElement = document.getElementById("log");

        function logMessage(message) {
            logElement.textContent += message + "\n";
            logElement.scrollTop = logElement.scrollHeight;
        }

        let audioContext, audioBuffers = {}, mediaStream, silenceDetectionActive = true;

        async function initAudio() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const audioFiles = ['Intro', 'Prompt1', 'Calm_response', 'Supportive_response'];
                for (const file of audioFiles) {
                    audioBuffers[file] = await loadAudioBuffer(file);
                }
                logMessage("Audio buffers loaded.");
            } catch (error) {
                logMessage("Error initializing audio context or loading buffers: " + error);
            }
        }

        async function loadAudioBuffer(fileName) {
            try {
                const response = await fetch(`https://raw.githubusercontent.com/Jsskieracer/Shifting-mvp/main/audio/${fileName}.mp3`);
                const arrayBuffer = await response.arrayBuffer();
                return await audioContext.decodeAudioData(arrayBuffer);
            } catch (error) {
                logMessage(`Error loading audio buffer for ${fileName}: ${error}`);
            }
        }

        async function playAudioBuffer(bufferName, delayAfter = 0) {
            try {
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffers[bufferName];
                source.connect(audioContext.destination);
                source.start();

                logMessage(`${bufferName} started playing.`);

                const bufferDuration = source.buffer.duration;
                await new Promise(resolve => setTimeout(resolve, (bufferDuration + delayAfter) * 1000));
                logMessage(`${bufferName} finished playing. Waiting for ${delayAfter} seconds.`);
            } catch (error) {
                logMessage(`Error playing ${bufferName}: ${error}`);
            }
        }

        document.getElementById('startButton').addEventListener('click', async () => {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                await audioContext.resume();
                logMessage("Audio context resumed.");
            }

            logMessage("Session starting...");

            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                logMessage("Microphone access granted");
            } catch (error) {
                logMessage("Microphone access denied: " + error);
                return;
            }

            await initAudio();
            await playAudioBuffer('Intro');
            await new Promise(resolve => setTimeout(resolve, 3000)); 
            logMessage("3-second delay completed after Intro.");
            await playAudioBuffer('Prompt1');

            logMessage("Prompt 1 segment finished; starting dynamic recording...");
            const userResponse = await captureDynamicInput();
            logMessage("User response captured: " + userResponse);

            const isCalm = await analyzeWithGPT(userResponse);
            if (isCalm) {
                logMessage("GPT analysis: User feels calm.");
                await playAudioBuffer('Calm_response');
            } else {
                logMessage("GPT analysis: User does not feel calm.");
                await playAudioBuffer('Supportive_response');
            }

            silenceDetectionActive = false;
            logMessage("Session complete; stopping silence detection logging.");
        });

        async function captureDynamicInput() {
            try {
                const recorder = new MediaRecorder(mediaStream);
                const chunks = [];
                let speakingDetected = false;
                const VOLUME_THRESHOLD = 15;
                const SILENCE_THRESHOLD = 5;

                recorder.ondataavailable = event => chunks.push(event.data);

                const analyser = audioContext.createAnalyser();
                const source = audioContext.createMediaStreamSource(mediaStream);
                source.connect(analyser);

                logMessage("Starting real-time silence detection.");

                const silenceDetectionPromise = new Promise(resolve => {
                    let silenceTimeout;

                    function detectSilence() {
                        if (!silenceDetectionActive) return;

                        const data = new Uint8Array(analyser.frequencyBinCount);
                        analyser.getByteFrequencyData(data);
                        const avgVolume = data.reduce((sum, value) => sum + value) / data.length;

                        if (avgVolume > VOLUME_THRESHOLD) {
                            if (!speakingDetected) {
                                recorder.start();
                                logMessage("Speech detected, recording started.");
                                speakingDetected = true;
                            }
                            clearTimeout(silenceTimeout);
                        } else if (speakingDetected) {
                            silenceTimeout = setTimeout(() => {
                                if (recorder.state === "recording") {
                                    recorder.stop();
                                    logMessage("Recording stopped after sustained silence.");
                                    resolve();
                                }
                            }, 3000); // Stop recording after 3 seconds of sustained silence
                        } else if (silenceDetectionActive) {
                            logMessage("Potential silence detected. Waiting for sustained silence...");
                        }

                        requestAnimationFrame(detectSilence);
                    }
                    detectSilence();
                });

                await silenceDetectionPromise;

                const audioBlob = await new Promise(resolve => {
                    recorder.onstop = () => resolve(new Blob(chunks, { type: 'audio/webm' }));
                });

                const audioBase64 = await blobToBase64(audioBlob);

                logMessage("Sending audio for transcription...");
                const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio: { content: audioBase64 },
                        config: { encoding: "WEBM_OPUS", sampleRateHertz: 48000, languageCode: "en-US" }
                    })
                });

                const data = await response.json();
                const transcription = data.results?.[0]?.alternatives?.[0]?.transcript || "No response detected.";
                logMessage("Transcription received: " + transcription);
                return transcription;

            } catch (error) {
                logMessage("Error capturing or transcribing audio: " + error);
                return "Error capturing audio.";
            }
        }

        async function analyzeWithGPT(transcription) {
    try {
        const gptResponse = await fetch(`https://api.openai.com/v1/completions`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`
            },
            body: JSON.stringify({
                model: "text-davinci-003",
                prompt: `Based on the following user response, determine if the user feels calm, has fewer thoughts, is feeling quiet or still. If any of these indicators are present, respond with "calm." Otherwise, respond with "not calm." Here is the user response:\n\n"${transcription}"`,
                max_tokens: 10,
                temperature: 0
            })
        });

        const gptData = await gptResponse.json();
        logMessage("Full GPT Response: " + JSON.stringify(gptData)); // Log the entire response for debugging

        const gptText = gptData.choices?.[0]?.text.trim().toLowerCase();
        if (!gptText) throw new Error("No valid response from GPT"); // Error handling if 'choices' is undefined

        logMessage("GPT analysis response: " + gptText);

        return gptText === "calm";
    } catch (error) {
        logMessage("Error analyzing response with GPT: " + error);
        return false;  // Default to "not calm" if GPT analysis fails
    }
}
        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => resolve(reader.result.split(",")[1]);
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }
    </script>
</body>
</html>
