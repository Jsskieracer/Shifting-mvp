<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
</head>
<body>
    <h1>Welcome to Your Shifting Session</h1>
    <p>Make sure you're in a quiet, comfortable space.</p>
    <button id="startButton">Start Session</button>

    <script>
        let mediaStream;
        let audio = new Audio();

        document.getElementById('startButton').addEventListener('click', async () => {
            console.log("Session starting...");

            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                console.log("Microphone access granted");
            } catch (error) {
                console.error("Microphone access denied:", error);
                return;
            }

            await playText("Take a deep breath in and out.");
            console.log("First prompt played.");
            await new Promise(resolve => setTimeout(resolve, 5000));

            await playText("What are you noticing? Is the mind quieting down or are other things happening?");
            console.log("Second prompt played.");

            const userResponse = await captureUserInputDynamic();
            console.log("User response captured:", userResponse);

            let responseText;
            if (checkForCalmResponse(userResponse)) {
                responseText = "Great, we’re complete.";
                console.log("Calm response detected, using default response.");
            } else {
                responseText = await getGptResponse(userResponse);
                console.log("GPT response received:", responseText);
            }

            await playText(responseText);
            console.log("Final response played.");
        });

        async function playText(text) {
            try {
                const url = `https://texttospeech.googleapis.com/v1/text:synthesize?key=AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`;
                const response = await fetch(url, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        input: { text: text },
                        voice: { languageCode: "en-US", name: "en-US-Wavenet-D" },
                        audioConfig: { audioEncoding: "MP3" }
                    })
                });

                const data = await response.json();
                const audioContent = data.audioContent;

                audio.src = `data:audio/mp3;base64,${audioContent}`;
                audio.load();
                await audio.play();
                console.log("Audio playback complete.");
            } catch (error) {
                console.error("Error in playText function:", error);
            }
        }

        async function captureUserInputDynamic() {
            try {
                const recorder = new MediaRecorder(mediaStream);
                const chunks = [];
                let speakingDetected = false;

                recorder.ondataavailable = event => chunks.push(event.data);

                const audioContext = new AudioContext();
                const source = audioContext.createMediaStreamSource(mediaStream);
                const analyser = audioContext.createAnalyser();
                source.connect(analyser);

                console.log("Starting real-time silence detection.");
                const silenceDetectionPromise = new Promise(resolve => {
                    function detectSilence() {
                        const data = new Uint8Array(analyser.frequencyBinCount);
                        analyser.getByteFrequencyData(data);
                        const avgVolume = data.reduce((sum, value) => sum + value) / data.length;

                        if (avgVolume > 10) {
                            if (!speakingDetected) {
                                recorder.start();
                                console.log("Speech detected, recording started.");
                                speakingDetected = true;
                            }
                            clearTimeout(silenceTimeout);
                        } else if (speakingDetected) {
                            silenceTimeout = setTimeout(() => {
                                if (recorder.state === "recording") {
                                    recorder.stop();
                                    console.log("Silence detected, recording stopped.");
                                    resolve();
                                }
                            }, 3000);
                        }
                        if (!speakingDetected || recorder.state === "recording") {
                            requestAnimationFrame(detectSilence);
                        }
                    }
                    detectSilence();
                });

                await silenceDetectionPromise;

                const audioBlob = await new Promise(resolve => {
                    recorder.onstop = () => resolve(new Blob(chunks, { type: 'audio/webm' }));
                });

                const audioBase64 = await blobToBase64(audioBlob);

                const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio: { content: audioBase64 },
                        config: { encoding: "WEBM_OPUS", sampleRateHertz: 48000, languageCode: "en-US" }
                    })
                });

                const data = await response.json();
                const transcription = data.results?.[0]?.alternatives?.[0]?.transcript || "No response detected.";
                console.log("Transcription received:", transcription);
                return transcription;

            } catch (error) {
                console.error("Error capturing or transcribing audio:", error);
                return "Error capturing audio.";
            }
        }

        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => resolve(reader.result.split(",")[1]);
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }

        async function getGptResponse(userText) {
            try {
                const url = `https://api.openai.com/v1/completions`;
                const response = await fetch(url, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`
                    },
                    body: JSON.stringify({
                        model: "text-davinci-003",
                        prompt: `User said: "${userText}". Respond with a gentle, supportive guidance based on the user's input.`,
                        max_tokens: 50,
                        temperature: 0.7
                    })
                });

                const data = await response.json();
                return data.choices[0].text.trim();

            } catch (error) {
                console.error("Error in getGptResponse function:", error);
                return "I'm here to support you. Let’s continue with some deep breaths.";
            }
        }

        function checkForCalmResponse(response) {
            const calmKeywords = ["calm", "peaceful", "relaxed", "quiet", "still"];
            const detected = calmKeywords.some(keyword => response.toLowerCase().includes(keyword));
            console.log("Calm response check:", detected);
            return detected;
        }
    </script>
</body>
</html>
