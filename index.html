<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
    <style>
        #log { 
            white-space: pre-wrap; 
            background-color: #f0f0f0; 
            padding: 10px; 
            border-radius: 5px; 
            max-height: 200px; 
            overflow-y: auto; 
            margin-top: 20px; 
        }
    </style>
</head>
<body>
    <h1>Welcome to Your Shifting Session</h1>
    <p>Make sure you're in a quiet, comfortable space and your AirPods are connected.</p>
    <button id="startButton">Start Session</button>
    <div id="log"></div>

    <script>
        const logElement = document.getElementById("log");

        function logMessage(message) {
            logElement.textContent += message + "\n";
            logElement.scrollTop = logElement.scrollHeight;
        }

        let audioContext, audioBuffers = {}, mediaStream;

        // Initialize Web Audio API and load audio buffers
        async function initAudio() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const audioFiles = ['Intro', 'Prompt1', 'CalmResponse', 'SupportiveResponse'];
            for (const file of audioFiles) {
                audioBuffers[file] = await loadAudioBuffer(file);
            }
            logMessage("Audio buffers loaded.");
        }

        // Load an audio file as a buffer
        async function loadAudioBuffer(fileName) {
            const response = await fetch(`https://raw.githubusercontent.com/Jsskieracer/Shifting-mvp/main/audio/${fileName}.m4a`);
            const arrayBuffer = await response.arrayBuffer();
            return await audioContext.decodeAudioData(arrayBuffer);
        }

        // Play an audio buffer with optional delay
        function playAudioBuffer(bufferName, delay = 0) {
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffers[bufferName];
            source.connect(audioContext.destination);
            source.start(audioContext.currentTime + delay);
            logMessage(`${bufferName} played successfully after ${delay} seconds.`);
        }

        document.getElementById('startButton').addEventListener('click', async () => {
            logMessage("Session starting...");

            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                logMessage("Microphone access granted");
            } catch (error) {
                logMessage("Microphone access denied: " + error);
                return;
            }

            // Initialize audio buffers and start session flow
            await initAudio();

            // Play Intro and Prompt1 with 5-second delay between
            playAudioBuffer('Intro');
            playAudioBuffer('Prompt1', 5); // Delay of 5 seconds after Intro

            // After Prompt1, capture user response
            logMessage("Prompt 1 segment should have played; moving to user input capture...");
            const userResponse = await captureUserInputDynamic();
            logMessage("User response captured: " + userResponse);

            // Check for a calm response and play the appropriate audio buffer
            if (checkForCalmResponse(userResponse)) {
                logMessage("Calm response detected; playing CalmResponse segment...");
                playAudioBuffer('CalmResponse');
            } else {
                logMessage("No calm response; playing SupportiveResponse segment...");
                playAudioBuffer('SupportiveResponse');
            }
        });

        async function captureUserInputDynamic() {
            try {
                const recorder = new MediaRecorder(mediaStream);
                const chunks = [];
                let speakingDetected = false;

                recorder.ondataavailable = event => chunks.push(event.data);

                const analyser = audioContext.createAnalyser();
                const source = audioContext.createMediaStreamSource(mediaStream);
                source.connect(analyser);

                logMessage("Starting real-time silence detection.");
                const silenceDetectionPromise = new Promise(resolve => {
                    function detectSilence() {
                        const data = new Uint8Array(analyser.frequencyBinCount);
                        analyser.getByteFrequencyData(data);
                        const avgVolume = data.reduce((sum, value) => sum + value) / data.length;

                        if (avgVolume > 10) {
                            if (!speakingDetected) {
                                recorder.start();
                                logMessage("Speech detected, recording started.");
                                speakingDetected = true;
                            }
                            clearTimeout(silenceTimeout);
                        } else if (speakingDetected) {
                            logMessage("Silence detected, setting timeout to stop recording...");
                            silenceTimeout = setTimeout(() => {
                                if (recorder.state === "recording") {
                                    recorder.stop();
                                    logMessage("Recording stopped after sustained silence.");
                                    resolve();
                                }
                            }, 3000); // Stop recording after 3 seconds of silence
                        }

                        requestAnimationFrame(detectSilence);
                    }
                    detectSilence();
                });

                await silenceDetectionPromise;

                const audioBlob = await new Promise(resolve => {
                    recorder.onstop = () => resolve(new Blob(chunks, { type: 'audio/webm' }));
                });

                const audioBase64 = await blobToBase64(audioBlob);

                const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=AIzaSyB9vYO8pDd_-2LlbRdpV-8FC0YzVYk9f_0`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio: { content: audioBase64 },
                        config: { encoding: "WEBM_OPUS", sampleRateHertz: 48000, languageCode: "en-US" }
                    })
                });

                const data = await response.json();
                const transcription = data.results?.[0]?.alternatives?.[0]?.transcript || "No response detected.";
                logMessage("Transcription received: " + transcription);
                return transcription;

            } catch (error) {
                logMessage("Error capturing or transcribing audio: " + error);
                return "Error capturing audio.";
            }
        }

        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => resolve(reader.result.split(",")[1]);
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }

        function checkForCalmResponse(response) {
            const calmKeywords = ["calm", "peaceful", "relaxed", "quiet", "still"];
            const detected = calmKeywords.some(keyword => response.toLowerCase().includes(keyword));
            logMessage("Calm response check: " + detected);
            return detected;
        }
    </script>
</body>
</html>
