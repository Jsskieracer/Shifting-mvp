<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shifting Session</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #F5F5DC; /* Soft beige background for calmness */
            color: #333;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        h1 {
            font-family: 'Georgia', serif;
            color: #4A8A7F; /* Soothing green */
            font-size: 2.5em;
        }

        p {
            font-size: 1.2em;
            color: #555;
            max-width: 600px;
            text-align: center;
            line-height: 1.6;
        }

        #startButton {
            font-size: 1.1em;
            color: #fff;
            background-color: #4A8A7F; /* Matching button color */
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
            margin-top: 20px;
        }

        #startButton.active {
            background-color: #FF6347; /* Red when active */
        }

        #log {
            white-space: pre-wrap;
            background-color: #e8e8e8;
            padding: 15px;
            border-radius: 5px;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 20px;
            width: 80%;
            font-family: monospace;
            font-size: 0.9em;
            color: #333;
        }

        .container {
            text-align: center;
            padding: 20px;
            max-width: 600px;
        }

        .background-image {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: url('https://www.example.com/your-calm-background.jpg'); /* Replace with a suitable image URL */
            background-size: cover;
            background-position: center;
            opacity: 0.2;
            z-index: -1;
        }
    </style>
</head>
<body>
    <div class="background-image"></div>
    <div class="container">
        <h1>Welcome to Your Shifting Session</h1>
        <p>Please ensure you're in a quiet, comfortable space to begin your journey.</p>
        <button id="startButton" onclick="toggleSession()">Start Session</button>
        <div id="log"></div>
    </div>

    <script>
        const logElement = document.getElementById("log");
        const startButton = document.getElementById("startButton");
        let audioContext, audioBuffers = {}, mediaStream, googleApiKey, silenceDetectionActive = true;

        function logMessage(message) {
            logElement.textContent += message + "\n";
            logElement.scrollTop = logElement.scrollHeight;
        }

        async function initAudio() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const audioFiles = ['Intro', 'Prompt1', 'Calm_response', 'Supportive_response'];
                for (const file of audioFiles) {
                    audioBuffers[file] = await loadAudioBuffer(file);
                }
                logMessage("Audio buffers loaded.");
            } catch (error) {
                logMessage("Error initializing audio context or loading buffers: " + error);
            }
        }

        async function loadAudioBuffer(fileName) {
            try {
                const response = await fetch(`https://raw.githubusercontent.com/Jsskieracer/Shifting-mvp/main/audio/${fileName}.mp3`);
                const arrayBuffer = await response.arrayBuffer();
                return await audioContext.decodeAudioData(arrayBuffer);
            } catch (error) {
                logMessage(`Error loading audio buffer for ${fileName}: ${error}`);
            }
        }

        async function playAudioBuffer(bufferName, delayAfter = 0) {
            try {
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffers[bufferName];
                source.connect(audioContext.destination);
                source.start();

                logMessage(`${bufferName} started playing.`);

                const bufferDuration = source.buffer.duration;
                await new Promise(resolve => setTimeout(resolve, (bufferDuration + delayAfter) * 1000));
                logMessage(`${bufferName} finished playing. Waiting for ${delayAfter} seconds.`);
            } catch (error) {
                logMessage(`Error playing ${bufferName}: ${error}`);
            }
        }

        async function toggleSession() {
            if (startButton.classList.contains("active")) {
                endSession();
                return;
            }
            startButton.classList.add("active");
            startButton.textContent = "Stop Session";

            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                await audioContext.resume();
                logMessage("Audio context resumed.");
            }

            logMessage("Session starting...");
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                logMessage("Microphone access granted");
            } catch (error) {
                logMessage("Microphone access denied: " + error);
                return;
            }

            googleApiKey = await fetchGoogleApiKey();
            if (!googleApiKey) {
                logMessage("Google API key could not be retrieved. Ending session.");
                endSession();
                return;
            }

            await initAudio();
            await playAudioBuffer('Intro');
            await new Promise(resolve => setTimeout(resolve, 3000));
            logMessage("3-second delay completed after Intro.");
            await playAudioBuffer('Prompt1');

            logMessage("Prompt 1 segment finished; starting dynamic recording...");
            const userResponse = await captureDynamicInput();
            logMessage("User response captured: " + userResponse);

            const isCalm = await analyzeWithGPT(userResponse);
            if (isCalm) {
                logMessage("GPT analysis: User feels calm.");
                await playAudioBuffer('Calm_response');
            } else {
                logMessage("GPT analysis: User does not feel calm.");
                await playAudioBuffer('Supportive_response');
            }

            endSession();
        }

        function endSession() {
            silenceDetectionActive = false;
            startButton.classList.remove("active");
            startButton.textContent = "Start Session";
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                logMessage("Microphone turned off.");
            }
            logMessage("Session complete; stopping silence detection logging.");
        }

        async function fetchGoogleApiKey() {
    try {
        logMessage("Starting fetch request for Google API key...");

        const response = await fetch("https://shifting-mvp.vercel.app/api/googleApiKey", {
            method: "GET",
            headers: {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
        });

        if (!response.ok) {
            logMessage(`Error fetching Google API key: HTTP ${response.status} ${response.statusText}`);
            throw new Error(`HTTP error! Status: ${response.status}`);
        }

        const data = await response.json();
        logMessage("Raw response data: " + JSON.stringify(data));

        if (data.googleApiKey) {
            logMessage("Google API Key retrieved successfully.");
            return data.googleApiKey;
        } else {
            throw new Error("Google API key not found in response.");
        }
    } catch (error) {
        logMessage("Error fetching Google API key: " + error.message);
        return null;
    }
}

        async function analyzeWithGPT(transcription) {
            try {
                const response = await fetch("https://shifting-mvp.vercel.app/api/OpenaiRequest", {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify({ transcription }),
                });

                const data = await response.json();
                logMessage("Full GPT Response: " + JSON.stringify(data)); 

                const gptText = data.choices?.[0]?.text.trim().toLowerCase();
                if (!gptText) {
                    throw new Error("Invalid response from GPT: No valid 'choices' data found.");
                }

                logMessage("GPT analysis response: " + gptText);
                return gptText === "calm";
            } catch (error) {
                logMessage("Error analyzing response with GPT: " + error.message);
                return false;
            }
        }

        async function captureDynamicInput() {
            try {
                const recorder = new MediaRecorder(mediaStream);
                const chunks = [];
                let speakingDetected = false;

                recorder.ondataavailable = event => chunks.push(event.data);

                const analyser = audioContext.createAnalyser();
                const source = audioContext.createMediaStreamSource(mediaStream);
                source.connect(analyser);

                logMessage("Starting real-time silence detection.");

                const silenceDetectionPromise = new Promise(resolve => {
                    let silenceTimeout;

                    function detectSilence() {
                        if (!silenceDetectionActive) return;

                        const data = new Uint8Array(analyser.frequencyBinCount);
                        analyser.getByteFrequencyData(data);
                        const avgVolume = data.reduce((sum, value) => sum + value) / data.length;

                        if (avgVolume > 10) {
                            if (!speakingDetected) {
                                recorder.start();
                                logMessage("Speech detected, recording started.");
                                speakingDetected = true;
                            }
                            clearTimeout(silenceTimeout);
                        } else if (speakingDetected) {
                            silenceTimeout = setTimeout(() => {
                                if (recorder.state === "recording") {
                                    recorder.stop();
                                    logMessage("Recording stopped after sustained silence.");
                                    resolve();
                                }
                            }, 3000);
                        }

                        requestAnimationFrame(detectSilence);
                    }
                    detectSilence();
                });

                await silenceDetectionPromise;

                const audioBlob = await new Promise(resolve => {
                    recorder.onstop = () => resolve(new Blob(chunks, { type: 'audio/webm' }));
                });

                const audioBase64 = await blobToBase64(audioBlob);

                logMessage("Sending audio for transcription...");
                const response = await fetch(`https://speech.googleapis.com/v1/speech:recognize?key=${googleApiKey}`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio: { content: audioBase64 },
                        config: { encoding: "WEBM_OPUS", sampleRateHertz: 48000, languageCode: "en-US" }
                    })
                });

                const data = await response.json();
                const transcription = data.results?.[0]?.alternatives?.[0]?.transcript || "No response detected.";
                logMessage("Transcription received: " + transcription);
                return transcription;

            } catch (error) {
                logMessage("Error capturing or transcribing audio: " + error);
                return "Error capturing audio.";
            }
        }

        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => resolve(reader.result.split(",")[1]);
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }
    </script>
</body>
</html>
